{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5 Benchmarking with `sktime`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbd704",
   "metadata": {},
   "source": [
    "this notebook: setting up reproducible forecasting benchmarking experiments with `sktime`.\n",
    "\n",
    "A benchmarking experiment is specified by:\n",
    "\n",
    "* one or multiple models, possibly highly composite pipelines as before!\n",
    "* evaluation metrics, e.g., MAPE, CRPS\n",
    "* data sets, e.g., M5 collection\n",
    "* re-sampling setup, e.g., expanding window splitter with certain parameters\n",
    "* fit/update specification\n",
    "* possibly, post-hoc analyses on results of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f77c01",
   "metadata": {},
   "source": [
    "for *reproducible* benchmarking, need to pass on the above information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c818a8",
   "metadata": {},
   "source": [
    "`sktime` makes this easy via:\n",
    "\n",
    "* persisting blueprints of composites, metrics, re-sampling set-ups\n",
    "* persisting fitted estimators if required\n",
    "* standard data access interfaces for common benchmark data\n",
    "* few-line set-up of benchmarking experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f899fc8",
   "metadata": {},
   "source": [
    "explained below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31600872",
   "metadata": {},
   "source": [
    "option 1:\n",
    "\n",
    "* python environment versions\n",
    "* jupyter notebook with experiment\n",
    "* any code for custom estimator classes\n",
    "\n",
    "option 2:\n",
    "\n",
    "* python environment versions\n",
    "* list of persisted object blueprints - estimators, metrics, cv\n",
    "* benchmark experiment setup params\n",
    "* any code for custom estimator classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1750d80",
   "metadata": {},
   "source": [
    "## 5.1 Persisting models and objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ba3ae",
   "metadata": {},
   "source": [
    "for reproducibility, one may like to share:\n",
    "\n",
    "* model blueprint specs, e.g., equivalent of spec `Pipeline([(\"foo\", Foo()), (\"bar\", Bar(42))])`\n",
    "* fitted models, e.g., state of `my_pipe.fit(y)` after the `fit` - specific to data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d16ee8",
   "metadata": {},
   "source": [
    "### 5.1.1 Persisting model blueprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d733376",
   "metadata": {},
   "source": [
    "blueprint specs can be serialized using simple string print - this contains all information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40207180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define an example pipeline\n",
    "from sktime.forecasting.compose._pipeline import TransformedTargetForecaster\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "pipe = TransformedTargetForecaster(\n",
    "    steps=[\n",
    "        (\"imputer\", Imputer()),\n",
    "        (\"forecaster\", NaiveForecaster()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0991da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the pipeline to a string\n",
    "# this is useful for logging and sharing\n",
    "# pipe_str can be saved to a file, database, or shared over the internet\n",
    "pipe_str = str(pipe)\n",
    "pipe_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27604a73",
   "metadata": {},
   "source": [
    "for pseudo-random determinism, set any `random_state` parameters in the estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec360734",
   "metadata": {},
   "source": [
    "to deserialize, use `registry.craft` in the same python environment\n",
    "\n",
    "for python environment, e.g., use `pip freeze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import craft\n",
    "\n",
    "pipe_new = craft(pipe_str)\n",
    "pipe_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfffddf",
   "metadata": {},
   "source": [
    "this is the same estimator blueprint as `pipe`!\n",
    "\n",
    "To compare blueprint, simply use the `==` operator (this is a `scikit-base` feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_new == pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7ed2a",
   "metadata": {},
   "source": [
    "share complex pipelines like this with your researcher friends (or in the appendix of your publications)!\n",
    "\n",
    "I.e., process as follows:\n",
    "\n",
    "* publishing researcher shares `pipe_str = str(pipe)` or `str(my_estimator)` and `pip freeze > requirements.txt` output\n",
    "* reproducing researcher installs env from `requirements.txt` and runs `craft(pipe_str)` in that env\n",
    "\n",
    "For custom estimators, in addition, the custom module needs to be shared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0672d7",
   "metadata": {},
   "source": [
    "Highly complex estimators can consist of multiple definition blocks - this is also supported by `craft` as follows.\n",
    "\n",
    "Instead of a string conversion, we can also serialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b721f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_spec is a string representation of the pipeline\n",
    "# it can be stored in a file or a database like this\n",
    "# the \"return\" statement indicates which object we store\n",
    "# temporary variables like pipe, cv can be defined\n",
    "pipe_spec = \"\"\"\n",
    "pipe = TransformedTargetForecaster(steps=[\n",
    "    (\"imputer\", Imputer()),\n",
    "    (\"forecaster\", NaiveForecaster())])\n",
    "cv = ExpandingWindowSplitter(\n",
    "    initial_window=24,\n",
    "    step_length=12,\n",
    "    fh=[1, 2, 3])\n",
    "\n",
    "return ForecastingGridSearchCV(\n",
    "    forecaster=pipe,\n",
    "    param_grid=[{\n",
    "        \"forecaster\": [NaiveForecaster(sp=12)],\n",
    "        \"forecaster__strategy\": [\"drift\", \"last\", \"mean\"],\n",
    "    },\n",
    "    {\n",
    "        \"imputer__method\": [\"mean\", \"drift\"],\n",
    "        \"forecaster\": [ThetaForecaster(sp=12)],\n",
    "    },\n",
    "    {\n",
    "        \"imputer__method\": [\"mean\", \"median\"],\n",
    "        \"forecaster\": [ExponentialSmoothing(sp=12)],\n",
    "        \"forecaster__trend\": [\"add\", \"mul\"],\n",
    "    },\n",
    "    ],\n",
    "    cv=cv,\n",
    "    n_jobs=-1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d58a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "craft(pipe_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5019fa",
   "metadata": {},
   "source": [
    "sometimes, estimators require soft dependencies to be installed,\n",
    "\n",
    "and complain at construction (or `craft`)\n",
    "\n",
    "for this, required dependencies can be queried *before* construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import deps\n",
    "\n",
    "deps(pipe_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc98de",
   "metadata": {},
   "source": [
    "... although this should not be necessary if `pip freeze` output is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87929231",
   "metadata": {},
   "source": [
    "another useful convenience: `imports` can be used to print a full import block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864dd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import imports\n",
    "\n",
    "imports(pipe_spec)  # the result can be copied above the spec in to a jupyter cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39fe09",
   "metadata": {},
   "source": [
    "### 5.1.2 Persisting fitted models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2f784",
   "metadata": {},
   "source": [
    "persisting fitted models can be useful to share in a reproducibility setting,\n",
    "\n",
    "(note, data source plus blueprint with `random_state` may be easier to share)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab450c88",
   "metadata": {},
   "source": [
    "to persist a fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_airline\n",
    "\n",
    "y = load_airline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a159e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example pipeline\n",
    "from sktime.forecasting.compose._pipeline import TransformedTargetForecaster\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "pipe = TransformedTargetForecaster(\n",
    "    steps=[\n",
    "        (\"imputer\", Imputer()),\n",
    "        (\"forecaster\", NaiveForecaster()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.fit(y, fh=[1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a47e1",
   "metadata": {},
   "source": [
    "to serialize fitted objects, use `save` - default is `pkl`, but may differ for deep learning\n",
    "\n",
    "* no args produces in-memory object\n",
    "* `str` or `Path` arg will serialize to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bdb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_mem = pipe.save()\n",
    "# pipe_mem is a pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0889ecc",
   "metadata": {},
   "source": [
    "to deserialize use the `load` method on the memory object or a `str`, `Path`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b24ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.base import load\n",
    "\n",
    "pipe_new = load(pipe_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255919c",
   "metadata": {},
   "source": [
    "the loaded object can be used for prediction now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dd21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_new.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.2 Forecast evaluation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.2.1 Metrics for point forecasts - basic workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf8ada",
   "metadata": {},
   "source": [
    "metrics are basic objects to compare actuals with forecasts.\n",
    "\n",
    "General usage pattern:\n",
    "\n",
    "1. get some actuals and forecasts\n",
    "2. specify the metric - similar to estimator specs\n",
    "3. plug the actuals and forecasts into metric to get metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99bd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get actuals and predictions\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "\n",
    "y = load_airline()\n",
    "\n",
    "y_train = y.iloc[:132]\n",
    "y_test = y.iloc[132:]  # actuals\n",
    "fh = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "forecaster = NaiveForecaster(strategy=\"last\", sp=12)\n",
    "\n",
    "forecaster.fit(y_train, fh=fh)\n",
    "y_pred = forecaster.predict()  # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658102dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "title = f\"actuals vs. predictions for {forecaster}\"\n",
    "fig, ax = plot_series(\n",
    "    y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"], title=title\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. specify the metric\n",
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
    "\n",
    "smape_loss = MeanAbsolutePercentageError(symmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40192af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. evaluate the metric \n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2e54a",
   "metadata": {},
   "source": [
    "important note: this single number should not be used as a point of comparison, in general it is high variance!\n",
    "\n",
    "better practice: backtesting experiment with sliding window, and optimally statistical hypothesis test\n",
    "\n",
    "that's what `evaluate` and `benchmarking` are for (below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f526406",
   "metadata": {},
   "source": [
    "side note: the `performance_metrics` module also contains \"loose functions\" for `sklearn` compatibility where needed, e.g., `mean_absolute_percentage_error`\n",
    "\n",
    "BUT we encourage the use of classes, because:\n",
    "\n",
    "* metric classes are first-class citizens in `sktime`, so\n",
    "* visible to `all_estimators` search\n",
    "* compatible with serialization workflows via `craft` or `save` (reproducibility!)\n",
    "* visible as parameter tunable components in estimators, e.g., grid search tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7c70c",
   "metadata": {},
   "source": [
    "to find metrics, use `all_estimators` with `\"metric\"` scitype for search:\n",
    "\n",
    "(or look in the online API reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2139bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import all_estimators\n",
    "\n",
    "all_estimators(\"metric\", as_dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f084f5",
   "metadata": {},
   "source": [
    "### 5.2.2 Metrics for point forecasts - advanced interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1026cb8",
   "metadata": {},
   "source": [
    "metrics classes have a number of advanced features:\n",
    "\n",
    "* automatic broadcasting to multivariate and hierarchical data\n",
    "* automatic coercion of any `sktime` compatible data formats\n",
    "* control of averaging over variables, time points, or multiple instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0149d9",
   "metadata": {},
   "source": [
    "let's get some multivariate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b061f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get actuals and predictions\n",
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "\n",
    "_, y = load_longley()\n",
    "\n",
    "y_train = y.iloc[:-3]\n",
    "y_test = y.iloc[-3:]  # actuals\n",
    "fh = [1, 2, 3]\n",
    "\n",
    "forecaster = NaiveForecaster(strategy=\"last\")\n",
    "\n",
    "forecaster.fit(y_train, fh=fh)\n",
    "y_pred = forecaster.predict()  # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c61700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. specify the metric\n",
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
    "\n",
    "smape_loss = MeanAbsolutePercentageError(symmetric=True)\n",
    "\n",
    "# 3. evaluate the metric\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a8161",
   "metadata": {},
   "source": [
    "... this also gives a number, by default:\n",
    "\n",
    "* averages over time points in `y_pred` / `y_test` (rows)\n",
    "* averages over variables (columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b69721",
   "metadata": {},
   "source": [
    "so, 3 rows and 5 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c386f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c6fdb",
   "metadata": {},
   "source": [
    "what if we don't want these averages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527dbb7f",
   "metadata": {},
   "source": [
    "variable (column) averaging is controlled by the `multioutput` arg.\n",
    "\n",
    "`\"raw_values\"` prevents averaging, `\"uniform_average\"` computes arithmetic mean.\n",
    "\n",
    "example without variable averaging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. specify the metric\n",
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
    "\n",
    "smape_loss_by_var = MeanAbsolutePercentageError(symmetric=True, multioutput=\"raw_values\")\n",
    "\n",
    "# 3. evaluate the metric\n",
    "smape_loss_by_var(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e54e8",
   "metadata": {},
   "source": [
    "evaluation by row can be useful for diagnostics or statistical tests\n",
    "\n",
    "for this, use the `evaluate_by_index` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fbea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_loss.evaluate_by_index(y_test, y_pred)\n",
    "# entries are the variable-averaged smape loss for each index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_loss_by_var.evaluate_by_index(y_test, y_pred)\n",
    "# entries are the variable-wise smape loss for each index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8207a6",
   "metadata": {},
   "source": [
    "Caveat: not every metric is an average over time points, e.g., RMSE\n",
    "\n",
    "In this case, `evaluate_by_index` computes jackknife pseudo-samples\n",
    "\n",
    "(for mean statistics, jackknife pseudo-samples are equal to individual samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce9bd9",
   "metadata": {},
   "source": [
    "let's look at hierarchical data & hierarchical level aggregation now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4febc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some hierarchical data\n",
    "from hierarchical_demo_utils import load_product_hierarchy\n",
    "\n",
    "y = load_product_hierarchy()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94500586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split y in train/test\n",
    "# note that this is hierarchical,\n",
    "# so we either do arcane pandas magic or use a splitter...\n",
    "from sktime.forecasting.model_selection import ExpandingGreedySplitter\n",
    "\n",
    "splitter = ExpandingGreedySplitter(3, folds=1)\n",
    "\n",
    "y_train, y_test = next(splitter.split_series(y))\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969baa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some predictions now\n",
    "fh = [1, 2, 3]\n",
    "\n",
    "forecaster = NaiveForecaster(strategy=\"last\")\n",
    "\n",
    "forecaster.fit(y_train, fh=fh)\n",
    "y_pred = forecaster.predict()  # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. specify the metric\n",
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
    "\n",
    "smape_loss = MeanAbsolutePercentageError(symmetric=True)\n",
    "\n",
    "# 3. evaluate the metric\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada83ba5",
   "metadata": {},
   "source": [
    "this has aggregated over hierarchy levels, by arithmetic mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa20fa",
   "metadata": {},
   "source": [
    "hierarchy level (row index level) averaging is controlled by the `multilevel` arg.\n",
    "\n",
    "`\"raw_values\"` prevents averaging, `\"uniform_average\"` computes arithmetic mean.\n",
    "\n",
    "example without hierarchy averaging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cfd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. specify the metric\n",
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError as MAPE\n",
    "\n",
    "smape_loss_by_level = MAPE(symmetric=True, multilevel=\"raw_values\")\n",
    "\n",
    "# 3. evaluate the metric\n",
    "smape_loss_by_level(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81d3e14",
   "metadata": {},
   "source": [
    "this can be combined with the other choices of averaging (or not):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9219f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_loss_by_level_and_variable = MAPE(\n",
    "    symmetric=True, multilevel=\"raw_values\", multioutput=\"raw_values\"\n",
    ")\n",
    "\n",
    "smape_loss_by_level_and_variable.evaluate_by_index(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5a27d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.2.3 Metrics for Probabilistic Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7420ce3",
   "metadata": {},
   "source": [
    "General usage pattern same as for point prediction metrics:\n",
    "\n",
    "1. get some actuals and forecasts\n",
    "2. specify the metric - similar to estimator specs\n",
    "3. plug the actuals and forecasts into metric to get metric values\n",
    "\n",
    "*but*: need to use dedicated metric for probabilistic forecasts\n",
    "\n",
    "* i.e., match metric with type of forecast!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac367dc",
   "metadata": {},
   "source": [
    "Recall methods available for all probabilistic forecasters:\n",
    "\n",
    "- `predict_interval` produces interval forecasts.\n",
    "  Argument `coverage` (nominal interval coverage) must be provided.\n",
    "- `predict_quantiles` produces quantile forecasts.\n",
    "  Argument `alpha` (quantile values) must be provided.\n",
    "- `predict_var` produces variance forecasts. Same args as `predict`.\n",
    "- `predict_proba` produces full distributional forecasts. Same args as `predict`.\n",
    "\n",
    "| Name | param | prediction/estimate of | `sktime` |\n",
    "| ---- | ----- | ---------------------- | -------- |\n",
    "| point forecast | | conditional expectation $\\mathbb{E}[y'\\|y]$ | `predict` |\n",
    "| variance forecast | | conditional variance $Var[y'\\|y]$ | `predict_var` |\n",
    "| quantile forecast | $\\alpha\\in (0,1)$ | $\\alpha$-quantile of $y'\\|y$ | `predict_quantiles` |\n",
    "| interval forecast | $c\\in (0,1)$| $[a,b]$ s.t. $P(a\\le y' \\le b\\| y) = c$ | `predict_interval` |\n",
    "| distribution forecast | | the law/distribution of $y'\\|y$ | `predict_proba` |\n",
    "\n",
    "where $y'$ is the forecast (considered a random variable) and $y$ is the observed history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281d9f9",
   "metadata": {},
   "source": [
    "let's produce some probabilistic forecasts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_shampoo_sales\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "\n",
    "y = load_shampoo_sales()\n",
    "y_train = y.iloc[:-4]\n",
    "y_test = y.iloc[-4:]\n",
    "\n",
    "forecaster = AutoETS(auto=True)\n",
    "fh = [1, 2, 3, 4]\n",
    "\n",
    "forecaster.fit(y_train, fh=fh)\n",
    "\n",
    "# use any of the probabilistic methods, we have seen this\n",
    "y_pred_int = forecaster.predict_interval(coverage=0.95)\n",
    "y_pred_q = forecaster.predict_quantiles(alpha=[0.05, 0.95])\n",
    "y_pred_proba = forecaster.predict_proba()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c78e5e",
   "metadata": {},
   "source": [
    "these now all have their own output format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_int  # lower/upper intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f521ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_q  # quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70576925",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba  # sktime/skpro BaseDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed59e0",
   "metadata": {},
   "source": [
    "we now need to apply a suitable metric, `metric(y_test, y_pred)`\n",
    "\n",
    "IMPORTANT: sequence matters, `y_test` first; `y_pred` has very different type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3292d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. specify metric\n",
    "# CRPS = continuous ranked probability score, for distribution predictions\n",
    "from sktime.performance_metrics.forecasting.probabilistic import CRPS\n",
    "\n",
    "crps = CRPS()\n",
    "\n",
    "# 3. evaluate metric\n",
    "crps(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc66d8",
   "metadata": {},
   "source": [
    "for averaging, same syntax as for point prediction metrics:\n",
    "\n",
    "* `multioutput` for averaging over variables or not, via `multioutput=\"raw_values\"`\n",
    "* `multilevel` for averaging over hierarchy levels or not\n",
    "* `evaluate_by_index` to return the loss by time index value (raw or pseudo-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no averaging over time points, variables\n",
    "from sktime.performance_metrics.forecasting.probabilistic import CRPS\n",
    "\n",
    "crps_no_avg = CRPS(multioutput=\"raw_values\")\n",
    "\n",
    "crps_no_avg.evaluate_by_index(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716cc08",
   "metadata": {},
   "source": [
    "there are a lot of metrics...\n",
    "\n",
    "how do we find a metric that fits the prediction type?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe0962",
   "metadata": {},
   "source": [
    "answer: metrics are tagged\n",
    "\n",
    "important tag: `scitype:y_pred`\n",
    "\n",
    "* `\"pred_proba\"` - distributional, can applied to distributions, `predict_proba` output\n",
    "* `\"pred_quantiles\"` - quantile forecast metric, can be applied to quantile predictions, interval predictions, distributional predictions\n",
    "    * applicable to `predict_quantiles`, `predict_interval`, `predict_proba` outputs\n",
    "* `\"pred_interval\"` - interval forecast metric, can be applied to interval predictions, distributional predictions\n",
    "    * applicable to `predict_interval`, `predict_proba` outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "crps.get_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c2dfd",
   "metadata": {},
   "source": [
    "listing metrics with the tag, filtering for probabilistic tags:\n",
    "\n",
    "(let's try to find a quantile forecast metric!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea65267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import all_estimators\n",
    "\n",
    "proba_tag_values = [\"pred_proba\", \"pred_interval\", \"pred_quantiles\"]\n",
    "all_estimators(\n",
    "    \"metric\",\n",
    "    as_dataframe=True,\n",
    "    filter_tags={\"scitype:y_pred\": proba_tag_values},\n",
    "    return_tags=\"scitype:y_pred\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f982c",
   "metadata": {},
   "source": [
    "`PinballLoss` is a quantile forecast metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting.probabilistic import PinballLoss\n",
    "\n",
    "pinball_loss = PinballLoss()\n",
    "\n",
    "pinball_loss(y_test, y_pred_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11849ad7",
   "metadata": {},
   "source": [
    "### 5.2.4 Time series splitters\n",
    "\n",
    "Tuning and evaluation uses data splitter schemas, e.g., expanding window splitter.\n",
    "\n",
    "`sktime` encodes data splitting schemas as objects of `\"splitter\"` type. Examples:\n",
    "\n",
    "- `SingleWindowSplitter`, single train-test-split\n",
    "- `SlidingWindowSplitter`, sliding window train set, followed by test set\n",
    "- `ExpandingWindowSplitter`, expanding window test set, followed by test set\n",
    "- `ExpandingGreedySplitter`, slices test set off from the end, rest is test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f72e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_shampoo_sales\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "\n",
    "y = load_shampoo_sales()\n",
    "y_train, y_test = temporal_train_test_split(y=y, test_size=6)\n",
    "plot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540960b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.model_selection import (\n",
    "    ExpandingGreedySplitter,\n",
    "    ExpandingWindowSplitter,\n",
    "    SlidingWindowSplitter,\n",
    "    SingleWindowSplitter,\n",
    ")\n",
    "from sktime.utils.plotting import plot_windows\n",
    "\n",
    "fh = ForecastingHorizon(y_test.index, is_relative=False).to_relative(\n",
    "    cutoff=y_train.index[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e69831",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = SingleWindowSplitter(fh=fh, window_length=len(y_train) - 6)\n",
    "plot_windows(cv=cv, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c658bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = SlidingWindowSplitter(fh=fh, window_length=12, step_length=1)\n",
    "plot_windows(cv=cv, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ExpandingGreedySplitter(test_size=4)\n",
    "plot_windows(cv=cv, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba76da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ExpandingWindowSplitter(fh=fh, initial_window=12, step_length=1)\n",
    "plot_windows(cv=cv, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of total splits (folds)\n",
    "cv.get_n_splits(y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a15e3",
   "metadata": {},
   "source": [
    "to list available splitters, use `all_estimators` with `\"splitter\"` type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import all_estimators\n",
    "\n",
    "all_estimators(\"splitter\", as_dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924eb4d5",
   "metadata": {},
   "source": [
    "### 5.2.5 Simple evaluation and backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110ffdf",
   "metadata": {},
   "source": [
    "Best practice for model evaluation: backtesting, sliding window\n",
    "\n",
    "(not single split MAPE etc...)\n",
    "\n",
    "how this works:\n",
    "\n",
    "* define backtesting schema using cross-validation splitter\n",
    "* simple workflows: use `evaluate` all-in-one evaluator\n",
    "    * raw evaluates by backtest split, estimator, metric\n",
    "* more advanced: use benchmarking module for experiments, next chapter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224510c0",
   "metadata": {},
   "source": [
    "data splitters (e.g., backtesting schemas) are first-class citizens in `sktime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ae4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_selection import ExpandingWindowSplitter\n",
    "from sktime.utils.plotting import plot_windows\n",
    "\n",
    "cv = ExpandingWindowSplitter(initial_window=24, fh=fh, step_length=2)\n",
    "n_folds = cv.get_n_splits(y)\n",
    "plot_windows(cv, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567b09d",
   "metadata": {},
   "source": [
    "Let's now run a simple evaluation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707030da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the data\n",
    "from sktime.datasets import load_shampoo_sales\n",
    "\n",
    "y = load_shampoo_sales()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587317ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the estimator\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "\n",
    "regressor = HistGradientBoostingRegressor()\n",
    "forecaster = make_reduction(regressor, strategy=\"direct\", window_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abe89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify evaluation metric\n",
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
    "\n",
    "smape_loss = MeanAbsolutePercentageError(symmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_evaluation import evaluate\n",
    "\n",
    "forecaster = forecaster.clone()\n",
    "scorers = smape_loss\n",
    "backtest = evaluate(forecaster=forecaster, y=y, cv=cv, scoring=scorers)\n",
    "backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fd11a",
   "metadata": {},
   "source": [
    "result is a table with:\n",
    "\n",
    "* rows = index of backtest splits from data splitter, here: expanding window split\n",
    "* aggregated performance metric (over variables, instances, time points)\n",
    "* runtimes for fit and predict\n",
    "* cutoff = most recent time point seen in train in that split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2699dc",
   "metadata": {},
   "source": [
    "`evaluate` can also:\n",
    "\n",
    "* compute multiple metrics\n",
    "* return data from individual splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e673808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_evaluation import evaluate\n",
    "from sktime.performance_metrics.forecasting import MeanSquaredError\n",
    "\n",
    "# another metric\n",
    "rmse = MeanSquaredError(square_root=True)\n",
    "\n",
    "forecaster = forecaster.clone()\n",
    "scorers = [smape_loss, rmse]\n",
    "backtest = evaluate(forecaster, y=y, cv=cv, scoring=scorers, return_data=True)\n",
    "backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103ac7a",
   "metadata": {},
   "source": [
    "## 5.3 Benchmarking - comparing estimator performance\n",
    "\n",
    "the `benchmarking` module allows you to set up experiments to:\n",
    "\n",
    "* compare the performance of one or more algorithms\n",
    "* over one or multiple datasets\n",
    "* against one or multiple performance metrics\n",
    "* for a benchmark configuration defined by temporal resampling scheme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819bf93",
   "metadata": {},
   "source": [
    "Benchmarking is very easy to get wrong...\n",
    "\n",
    "see this [2022 research from Princeton](https://reproducible.cs.princeton.edu/)\n",
    "for numerous examples of such mistakes in peer reviewed papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1cbe3f",
   "metadata": {},
   "source": [
    "`sktime`'s `benchmarking` module is designed to:\n",
    "\n",
    "* provide a high-level specification language\n",
    "* prevent mistakes by abstracting away \"dangerous\" implementation details\n",
    "* allow reproducible sharing of experiment setups and results\n",
    "\n",
    "Any `sktime` compatible object can be plugged in!\n",
    "\n",
    "* forecasters, transformers in pipelines, etc\n",
    "* performance metrics\n",
    "* cross-validation schemas\n",
    "* datasets in `sktime` compatible formats\n",
    "\n",
    "Use the `sktime` extension templates to add your favourite objects to the experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218875c",
   "metadata": {},
   "source": [
    "advanced forecasting benchmark workflow, using `benchmark` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.benchmarking.forecasting import ForecastingBenchmark\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.model_selection import ExpandingWindowSplitter\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.performance_metrics.forecasting import MeanSquaredPercentageError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b19c7",
   "metadata": {},
   "source": [
    "### Instantiate an instance of a benchmark class\n",
    "In this example we are comparing forecasting estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed25ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = ForecastingBenchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd5032",
   "metadata": {},
   "source": [
    "### Add competing estimators\n",
    "We add different competing estimators to the benchmark instance. All added estimators will \n",
    "be automatically ran through each added benchmark tasks, and their results compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9122963",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.add_estimator(\n",
    "    estimator=NaiveForecaster(strategy=\"mean\", sp=12),\n",
    "    estimator_id=\"NaiveForecaster-mean-v1\",\n",
    ")\n",
    "benchmark.add_estimator(\n",
    "    estimator=NaiveForecaster(strategy=\"last\", sp=12),\n",
    "    estimator_id=\"NaiveForecaster-last-v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30056814",
   "metadata": {},
   "source": [
    "### Add benchmarking tasks\n",
    "These are the prediction/validation tasks over which every estimator will be tested and their results compiled.\n",
    "\n",
    "The exact arguments for a benchmarking task depend on the whether the objective is forecasting, classification, etc.,\n",
    "but generally they are similar. The following are the required arguments for defining a forecasting benchmark task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368d276",
   "metadata": {},
   "source": [
    "#### Specify cross-validation split regime(s)\n",
    "Define cross-validation split regimes, using standard `sktime` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812bd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_splitter = ExpandingWindowSplitter(\n",
    "    initial_window=24,\n",
    "    step_length=12,\n",
    "    fh=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e2a07",
   "metadata": {},
   "source": [
    "#### Specify performance metric(s)\n",
    "Define performance metrics on which to compare estimators, using standard `sktime` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = [MeanSquaredPercentageError()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b66d3",
   "metadata": {},
   "source": [
    "#### Specify dataset loaders\n",
    "Define dataset loaders, which are callables (functions) which should return a dataset. Generally\n",
    "this is a callable which returns a dataframe containing the entire dataset. One can use\n",
    "the `sktime` defined datasets, or define their own. Something as simple as the following\n",
    "example will suffice: \n",
    "```python\n",
    "def my_dataset_loader():\n",
    "    return pd.read_csv(\"path/to/data.csv\")\n",
    "```\n",
    "The datasets will be loaded when running the benchmarking tasks, ran through the cross-validation\n",
    "regime(s) and subsequently the estimators will be tested over the dataset splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loaders = [load_airline]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499be64",
   "metadata": {},
   "source": [
    "#### Add tasks to the benchmark instance\n",
    "Use the previously defined objects to add tasks to the benchmark instance.\n",
    "Optionally use loops etc. to easily setup multiple benchmark tasks reusing arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_loader in dataset_loaders:\n",
    "    benchmark.add_task(\n",
    "        dataset_loader,\n",
    "        cv_splitter,\n",
    "        scorers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401deb16",
   "metadata": {},
   "source": [
    "### Run all task-estimator combinations and store results\n",
    "\n",
    "Note that `run` won't rerun tasks it already has results for, so adding a new\n",
    "estimator and running `run` again will only run tasks for that new estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = benchmark.run(\"./forecasting_results.csv\")\n",
    "results_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Credits: notebook 3 - Metrics and Evaluation\n",
    "\n",
    "notebook creation:\n",
    "\n",
    "forecaster pipelines:\n",
    "transformer pipelines & compositors:\n",
    "dunder interface:\n",
    "\n",
    "tuning, autoML:\n",
    "CV and splitters:\n",
    "forecasting metrics:\n",
    "backtesting, evaluation:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
