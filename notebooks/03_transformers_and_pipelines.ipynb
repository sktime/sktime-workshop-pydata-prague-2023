{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple forecasting pipelines in `sktime`\n",
    "\n",
    "Until now: How can we use sktime to create forecasts.\n",
    "\n",
    "\n",
    "Now: How can we improve the forecasts.\n",
    "\n",
    "* endogenous transformation pipelines via `TransformedTargetForecaster`\n",
    "* exogenous transformation pipelines via `ForecastingPipeline`\n",
    "* feature extraction compositors\n",
    "    * `FeatureUnion` and `ColumnSelect` for multiple feature extraction\n",
    "    * `ColumnEnsembleForecaster` and `ColumnEnsembleTransformer` to apply per variable\n",
    "* forecasting exogenous variables via `ForecastX`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Introducing transformers for forecasting pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or: why transformer estimators are the \"right\" concept to modularize forecasting pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose we want to forecast this well-known dataset\n",
    "(airline passengers by year in a fixed scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_airline\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "y = load_airline()\n",
    "plot_series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observations:\n",
    "\n",
    "* there is seasonal periodicity, 12 month period\n",
    "* seasonal periodicity looks multiplicative (not additive) to trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: forecast might be easier\n",
    "\n",
    "* with seasonality removed\n",
    "* on logarithmic value scale (multiplication becomes addition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Manual tranformations: doing things the wrong way\n",
    "\n",
    "Maybe doing this manually step by step is a good idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_log = np.log(y)\n",
    "\n",
    "fig, ax = plot_series(y_log, title=\"log(y)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this looks additive now!\n",
    "\n",
    "ok, what next - deaseasonalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "seasonal_result = seasonal_decompose(y_log, period=12)\n",
    "seasonal = seasonal_result.seasonal\n",
    "y_log_deseasonalised = y_log - seasonal\n",
    "\n",
    "fig, ax = plot_series(y_log_deseasonalised, title=\"log(y) - seasonality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = seasonal_result.trend\n",
    "plot_series(trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = seasonal_result.resid\n",
    "plot_series(seasonal, resid, labels=[\"seasonal component\", \"residual component\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now:\n",
    "\n",
    "* forecast on this\n",
    "* add back seasonal component\n",
    "* invert logarithm (exponentiate)\n",
    "\n",
    "start with forecast..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.trend import TrendForecaster\n",
    "\n",
    "forecaster = TrendForecaster()\n",
    "\n",
    "fh = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] # Alternatively: list(range(1, 13))\n",
    "y_pred = forecaster.fit_predict(y_log_deseasonalised, fh=fh)\n",
    "\n",
    "fig, ax = plot_series(\n",
    "    y_log_deseasonalised,\n",
    "    y_pred,\n",
    "    labels=[\"log(y) - seasonality\", \"y_pred\"],\n",
    "    title=\"Trend forecast over log(y) - seasonality\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks reasonable!\n",
    "\n",
    "Now to turn this into a forecast of the original y ...\n",
    "\n",
    "* add seasonal\n",
    "* invert the logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_add_seasonality = y_pred + seasonal[0:12].values\n",
    "y_pred_orig = np.exp(y_pred_add_seasonality)\n",
    "\n",
    "fig, ax = plot_series(y, y_pred_orig, labels=[\"y\", \"y_pred\"], title=\"Final forecast (manual approach)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, done! and it only took us 10 years.\n",
    "\n",
    "Maybe there is a better way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 `sktime` transformers: doing things the right way\n",
    "\n",
    "\n",
    "Solution: use transformers & pipelines!\n",
    "\n",
    "Same interface at every step! Easily composable!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.trend import TrendForecaster\n",
    "from sktime.transformations.series.boxcox import LogTransformer\n",
    "from sktime.transformations.series.detrend import Deseasonalizer\n",
    "\n",
    "y = load_airline()\n",
    "\n",
    "forecaster = LogTransformer() * Deseasonalizer(sp=12) * TrendForecaster()\n",
    "\n",
    "fh = list(range(1, 13))\n",
    "y_pred = forecaster.fit_predict(y, fh=fh)\n",
    "\n",
    "fig, ax = plot_series(y, y_pred, labels=[\"y\", \"y_pred\"], title=\"Final forecast with sktime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what happened here?\n",
    "\n",
    "The \"chain\" operator `*` creates a \"forecasting pipeline\"\n",
    "\n",
    "Has the same interface as all other forecasters! No additional data fiddling!\n",
    "\n",
    "Transformers \"slot in\" as standardized components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this in more detail:\n",
    "\n",
    "* `sktime` transformers interface\n",
    "* `sktime` pipeline building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Transformers - More Detailed\n",
    "\n",
    "* transformer interface\n",
    "* transformer types\n",
    "* searching transformers by type\n",
    "* broadcasting/vectorization to panel & hierarchical data\n",
    "* transformers and pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 What are transformers? <a class=\"anchor\" id=\"section_1_1\"></a>\n",
    "\n",
    "Transformer = modular data processing steps commonly used in machine learning\n",
    "\n",
    "(\"transformer\" used in the sense of `scikit-learn`)\n",
    "\n",
    "Transformers are estimators that:\n",
    "\n",
    "* are fitted to a batch of data via `fit(data)`, changing its state\n",
    "* are applied to another batch of data via `transform(X)`, producing transformed data\n",
    "* may have an `inverse_transform(X)`\n",
    "\n",
    "In `sktime`, input `X` to `fit` and `transform` is typically a time series or a panel (collection of time series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic use of an `sktime` time series transformer is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. prepare the data\n",
    "from sktime.utils._testing.series import _make_series\n",
    "\n",
    "X = _make_series()\n",
    "X_train = X[:7]\n",
    "X_test = X[7:12]\n",
    "# X_train and X_test are both pandas.Series\n",
    "\n",
    "X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. construct the transformer\n",
    "from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
    "\n",
    "# trafo is an sktime estimator inheriting from BaseTransformer\n",
    "# Box-Cox transform with lambda parameter fitted via mle\n",
    "trafo = BoxCoxTransformer(method=\"mle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. fit the transformer to training data\n",
    "trafo.fit(X_train)\n",
    "\n",
    "# 4. apply the transformer to transform test data\n",
    "# Box-Cox transform with lambda fitted on X_train\n",
    "X_transformed = trafo.transform(X_test)\n",
    "\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training and test set is the same, step 3 and 4 can be carried out more concisely (and sometimes more efficiently) by using `fit_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3+4. apply the transformer to fit and transform on the same data, X\n",
    "X_transformed = trafo.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For time series transformers, the metadata tags describe the expected output of `transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafo.get_tag(\"scitype:transform-output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafo.get_tag(\"scitype:transform-output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find transformers, use `all_estimators` and filter by tags:\n",
    "\n",
    "* `\"scitype:transform-output\"` - the output scitype. `Series` for time series, `Primitives` for primitive features (float, categories), `Panel` for collections of time series.\n",
    "* `\"scitype:transform-input\"` - the input scitype. `Series` for time series.\n",
    "* `\"scitype:instancewise\"` - If `True`, vectorized operation per series. If `False`, uses multiple time series non-trivially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: find all transformers that transform time series to time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import all_estimators\n",
    "\n",
    "# now subset to transformers that extract scalar features\n",
    "all_estimators(\n",
    "    \"transformer\",\n",
    "    as_dataframe=True,\n",
    "    filter_tags={\"scitype:transform-output\": \"Series\"},\n",
    "    suppress_import_stdout=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complete overview on transformer types and tags is given in the `sktime` transformers tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Hierarchical Transformers\n",
    "\n",
    "* Either transformers are natively multivariate or\n",
    "* If not: Broadcasting aka vectorization of transformers\n",
    "\n",
    "This ensures that all `sktime` transformers can be applied to multivariate and multi-instance (panel, hierarchical) time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: broadcasting/vectorization of time series to time series transformer\n",
    "\n",
    "The `BoxCoxTransformer` from previous sections applies to single instances of univariate time series. When multiple instances or variables are seen, it broadcasts across both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
    "from sktime.utils._testing.hierarchical import _make_hierarchical\n",
    "\n",
    "# hierarchical data with 2 variables and 2 levels\n",
    "X = _make_hierarchical(n_columns=2)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the transformers\n",
    "boxcox_trafo = BoxCoxTransformer(method=\"mle\")\n",
    "\n",
    "# applying to X results in hierarchical data\n",
    "boxcox_trafo.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitted model components of vectorized transformers can be found in the `transformers_` attribute, or accessed via the universal `get_fitted_params` interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxcox_trafo.transformers_\n",
    "# this is a pandas.DataFrame that contains the fitted transformers\n",
    "# one per time series instance and variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxcox_trafo.get_fitted_params()\n",
    "# this returns a dictionary\n",
    "# the transformers DataFrame is available at the key \"transformers\"\n",
    "# individual transformers are available at dataframe-like keys\n",
    "# it also contains all fitted lambdas as keyed parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: broadcasting/vectorization of time series to scalar features transformer\n",
    "\n",
    "The `SummaryTransformer` behaves similarly.\n",
    "Multiple time series instances are transformed to different columns of the resulting data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.summarize import SummaryTransformer\n",
    "\n",
    "summary_trafo = SummaryTransformer()\n",
    "\n",
    "# this produces a pandas DataFrame with more rows and columns\n",
    "# rows correspond to different instances in X\n",
    "# columns are multiplied and names prefixed by [variablename]__\n",
    "# there is one column per variable and transformed feature\n",
    "summary_trafo.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sequential Pipelines, Feature Engineering\n",
    "\n",
    "`sktime` transformers can be pipelined with any other `sktime` estimator type, including forecasters, classifiers, and other transformers.\n",
    "\n",
    "Pipelines = estimators of the same type, same interface as specialized class\n",
    "\n",
    "pipeline build operation: `make_pipeline` or via `*` dunder\n",
    "\n",
    "Pipelining `pipe = trafo * est` produces `pipe` of same type as `est`.\n",
    "\n",
    "In `pipe.fit`, first `trafo.fit_transform`, then `est.fit` is executed on the result.\n",
    "\n",
    "In `pipe.predict`, first `trafo.transform`, then `est.predict` is executed.\n",
    "\n",
    "(the arguments that are piped differ by type and can be looked up in the docstrings of pipeline classes, or specialized tutorials)\n",
    "\n",
    "\n",
    "transformers are natural pipeline components\n",
    "\n",
    "* data processing steps\n",
    "* feature engineering steps\n",
    "* post processing steps\n",
    "\n",
    "they can be combined in a number of other ways:\n",
    "\n",
    "* pipelining = sequential chaining\n",
    "* feature union = parallel, addition of features\n",
    "* feature subsetting = selecting columns\n",
    "* inversion = switch transform and inverse\n",
    "* multiplexing = switching between transformers\n",
    "* passthrough = switch on/ off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Chaining transformers via `*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(or `TransformedTargetForecaster`)\n",
    "\n",
    "Let's re-examine the pipeline ethat we have seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.trend import TrendForecaster\n",
    "from sktime.transformations.series.boxcox import LogTransformer\n",
    "from sktime.transformations.series.detrend import Deseasonalizer\n",
    "\n",
    "pipe = LogTransformer() * Deseasonalizer(sp=12) * TrendForecaster()\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`*` applies transformers to the endogenous data as follows:\n",
    "\n",
    "* in `fit`, does `fit_transform`, then passes on\n",
    "* in `predict`, takes input, then does `inverse_transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipe` is a `TransformedTargetForecaster`, which we could also have specified as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "\n",
    "pipe_explicit = TransformedTargetForecaster(\n",
    "    [\n",
    "        (\"log\", LogTransformer()),\n",
    "        (\"deseasonalise\", Deseasonalizer(sp=12)),\n",
    "        (\"forecast\", TrendForecaster()),\n",
    "    ]\n",
    ")\n",
    "pipe_explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`*` dunder automatically constructs the \"right\" pipeline, compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogTransformer() * Deseasonalizer(sp=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is now a `TransformerPipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline building is compatible with sklearn transformers!\n",
    "\n",
    "default applies sklearn transformer per individual time series as a data frame table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe_with_sklearn = Deseasonalizer(sp=12) * StandardScaler() * TrendForecaster()\n",
    "pipe_with_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `StandardScaler` is wrapped in `transformations.series.adapt.TabularToSeriesAdaptor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "composites are compatible with `get_params` / `set_params` parameter interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Transforming exogenous data via `**`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or `ForecastingPipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`**` applies transformers to the exogenous data, `X`:\n",
    "\n",
    "* in `fit`, applies `fit_transform` to `X` and passes it on\n",
    "* in `predict`, applies `transform` to `X` and passes it on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "forecaster = NaiveForecaster(strategy=\"drift\")\n",
    "imputer = Imputer(method=\"mean\")\n",
    "\n",
    "pipe = imputer ** MinMaxScaler() ** forecaster\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also bracket - then, important, use `*` for transformer composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = (imputer * MinMaxScaler()) ** forecaster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"verbose\" definition for configurability of names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.compose import ForecastingPipeline\n",
    "from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "\n",
    "pipe_explicit = ForecastingPipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", Imputer(method=\"mean\")),\n",
    "        (\"minmaxscaler\", TabularToSeriesAdaptor(MinMaxScaler())),\n",
    "        (\"forecaster\", NaiveForecaster(strategy=\"drift\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Feature union via `+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's say we waqnt to extract features:\n",
    "\n",
    "* first differences of time series\n",
    "* first lag of time series\n",
    "\n",
    "of the exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.difference import Differencer\n",
    "from sktime.transformations.series.lag import Lag\n",
    "\n",
    "pipe = Differencer() + Lag(1)\n",
    "\n",
    "# this constructs a FeatureUnion, which is also a transformer\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n",
    "\n",
    "X = _bottom_hier_datagen(no_levels=1, no_bottom_nodes=2)\n",
    "\n",
    "# applies both Differencer and Lag, returns transformed in different columns\n",
    "pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to retain the original columns, use the `Id` transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.compose import Id\n",
    "from sktime.transformations.series.difference import Differencer\n",
    "from sktime.transformations.series.lag import Lag\n",
    "\n",
    "pipe = Id() + Differencer() + Lag([1, 2], index_out=\"original\")\n",
    "\n",
    "pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter inspection\n",
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Subset input columns via `[colname]`\n",
    "\n",
    "let's say we want to apply `Differencer` to column 0, and `Lag` to column 1\n",
    "\n",
    "also we keep the original columns for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils._testing.hierarchical import _make_hierarchical\n",
    "\n",
    "X = _make_hierarchical(\n",
    "    hierarchy_levels=(2, 2), n_columns=2, min_timepoints=3, max_timepoints=3\n",
    ")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.compose import Id\n",
    "from sktime.transformations.series.difference import Differencer\n",
    "from sktime.transformations.series.lag import Lag\n",
    "\n",
    "pipe = Id() + Differencer()[\"c0\"] + Lag([1, 2], index_out=\"original\")[\"c1\"]\n",
    "\n",
    "pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auto-generated names can be replaced by using `FeatureUnion` explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.compose import FeatureUnion\n",
    "\n",
    "pipe = FeatureUnion(\n",
    "    [\n",
    "        (\"original\", Id()),\n",
    "        (\"diff\", Differencer()[\"c0\"]),\n",
    "        (\"lag\", Lag([1, 2], index_out=\"original\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see more later in part 3 on how to use this with tuning for full structural AutoML!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3.5 ColumnEnsemleTransformer and ColumnEnsembleForecaster\n",
    "\n",
    "Apply different forecasters or transformers to different columns of the time series:\n",
    "\n",
    "\n",
    "Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.compose import ColumnEnsembleTransformer\n",
    "from sktime.transformations.series.detrend import Detrender\n",
    "from sktime.transformations.series.difference import Differencer\n",
    "from sktime.datasets import load_longley\n",
    "\n",
    "\n",
    "y = load_longley()[1][[\"GNP\", \"UNEMP\"]]\n",
    "\n",
    "transformer = ColumnEnsembleTransformer([(\"difference\", Differencer(), 1),\n",
    "                                         (\"trend\", Detrender(), 0),\n",
    "                                        ])\n",
    "\n",
    "transformer.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecaster: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.compose import ColumnEnsembleForecaster\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.datasets import load_longley\n",
    "\n",
    "y = load_longley()[1][[\"GNP\", \"UNEMP\"]]\n",
    "forecasters = [\n",
    "    (\"trend\", PolynomialTrendForecaster(), 0),\n",
    "    (\"naive\", NaiveForecaster(), 1),\n",
    "]\n",
    "forecaster = ColumnEnsembleForecaster(forecasters=forecasters)\n",
    "forecaster.fit(y, fh=[1, 2, 3])\n",
    "y_pred = forecaster.predict()\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 Forecasting using signal decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above tools, we can build a pipeline which does the following:\n",
    "\n",
    "* decompose endogenous signal `y`, e.g., into trend, period (plus noise component)\n",
    "* forecast these components with different forecasters\n",
    "* recompose component forecasts to overall forecast\n",
    "\n",
    "works with any signal decomposition transformer!\n",
    "E.g., `VmdTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo vmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Forecaster Pipeline Components to control exogenous variables\n",
    "\n",
    "pipeline elements for granular control around exogenous variables:\n",
    "\n",
    "* forecasting exogenous variables - `ForecastX`\n",
    "* creating exogenous variables from endogenous variables - `YtoX`\n",
    "* predicting endogenous variables using exogenous variables - `YfromX`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.4.1 Forecasting Exogenous Variables\n",
    "\n",
    "Sometimes a forecaster uses exogenous features that are itself forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "y, X = load_longley()\n",
    "y_train, y_test, X_train, X_test = temporal_train_test_split(y=y, X=X, test_size=6)\n",
    "fh = ForecastingHorizon(y_test.index, is_relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sktime.forecasting.compose import ForecastX\n",
    "from sktime.forecasting.var import VAR\n",
    "\n",
    "forecaster_X = ForecastX(\n",
    "    forecaster_y=AutoARIMA(sp=1, suppress_warnings=True),\n",
    "    forecaster_X=VAR(),\n",
    ")\n",
    "forecaster_X.fit(y=y, X=X, fh=fh)\n",
    "# now in predict() we don't need to pass X\n",
    "y_pred = forecaster_X.predict(fh=fh)\n",
    "\n",
    "plot_series(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.4 Tuning\n",
    "* Lot of hyperparameters in a pipeline. We want to optimise them:\n",
    "\n",
    "### 1.4.1 Temporal Cross Validation\n",
    "\n",
    "`sktime` encodes data splitting schemas as objects of `\"splitter\"` type. Examples:\n",
    "\n",
    "- `SingleWindowSplitter`, single train-test-split\n",
    "- `SlidingWindowSplitter`, sliding window train set, followed by test set\n",
    "- `ExpandingWindowSplitter`, expanding window test set, followed by test set\n",
    "- `ExpandingGreedySplitter`, slices test set off from the end, rest is test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sktime.datasets import load_shampoo_sales\n",
    "\n",
    "y = load_shampoo_sales()\n",
    "y_train, y_test = temporal_train_test_split(y=y, test_size=6)\n",
    "plot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.model_selection import (\n",
    "    ExpandingWindowSplitter,\n",
    "    SlidingWindowSplitter,\n",
    "    SingleWindowSplitter,\n",
    ")\n",
    "from sktime.utils.plotting import plot_windows\n",
    "\n",
    "fh = ForecastingHorizon(y_test.index, is_relative=False).to_relative(\n",
    "    cutoff=y_train.index[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = SingleWindowSplitter(fh=fh, window_length=len(y_train) - 6)\n",
    "plot_windows(cv=cv, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = SlidingWindowSplitter(fh=fh, window_length=12, step_length=1)\n",
    "plot_windows(cv=cv, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = ExpandingWindowSplitter(fh=fh, initial_window=12, step_length=1)\n",
    "plot_windows(cv=cv, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get number of total splits (folds)\n",
    "cv.get_n_splits(y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to list available splitters, use `all_estimators` with `\"splitter\"` type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import all_estimators\n",
    "\n",
    "all_estimators(\"splitter\", as_dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.4.2 Tuning via grid Search\n",
    "\n",
    "For tuning parameters with compositions such as pipelines, we can use the \\<estimator\\>__\\<parameter\\> syntax known from [scikit-learn](https://scikit-learn.org/stable/modules/grid_search.html#composite-estimators-and-parameter-spaces). For multiple levels of nesting, we can use the same syntax with two underscores, e.g. `forecaster__transformer__parameter`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer, RobustScaler, MinMaxScaler\n",
    "from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.model_selection import ForecastingGridSearchCV\n",
    "from sktime.performance_metrics.forecasting import MeanSquaredError\n",
    "from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
    "\n",
    "forecaster = TransformedTargetForecaster(\n",
    "    steps=[\n",
    "        (\"detrender\", Detrender()),\n",
    "        (\"deseasonalizer\", Deseasonalizer()),\n",
    "        (\"minmax\", TabularToSeriesAdaptor(MinMaxScaler((1, 10)))),\n",
    "        (\"power\", TabularToSeriesAdaptor(PowerTransformer())),\n",
    "        (\"scaler\", TabularToSeriesAdaptor(RobustScaler())),\n",
    "        (\"forecaster\", ExponentialSmoothing()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# using dunder notation to access inner objects/params as in sklearn\n",
    "param_grid = {\n",
    "    # deseasonalizer\n",
    "    \"deseasonalizer__model\": [\"multiplicative\", \"additive\"],\n",
    "    # power\n",
    "    \"power__transformer__method\": [\"yeo-johnson\", \"box-cox\"],\n",
    "    \"power__transformer__standardize\": [True, False],\n",
    "    # forecaster\n",
    "    \"forecaster__sp\": [4, 6, 12],\n",
    "    \"forecaster__seasonal\": [\"add\", \"mul\"],\n",
    "    \"forecaster__trend\": [\"add\", \"mul\"],\n",
    "    \"forecaster__damped_trend\": [True, False],\n",
    "}\n",
    "\n",
    "gscv = ForecastingGridSearchCV(\n",
    "    forecaster=forecaster,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring=MeanSquaredError(square_root=True),  # set custom scoring function\n",
    ")\n",
    "gscv.fit(y_train)\n",
    "y_pred = gscv.predict(fh=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gscv.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gscv.best_forecaster_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gscv.cv_results_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.5 Summary<a class=\"anchor\" id=\"chapter5\"></a>\n",
    "\n",
    "* transformers are data processing steps with unified interface - `fit`, `transform`, and optional `inverse_transform`\n",
    "\n",
    "* used as pipeline components for any learning task, forecasting, classification\n",
    "\n",
    "* different types by input/output - time series, primitives, pairs of time series, panels/hierarchical.\n",
    "\n",
    "* find transformers by tags such as `scitype:transform-output` and `scitype:instancewise` using `all_estimators`\n",
    "\n",
    "* rich composition syntax - `*` for pipe, `+` for featureunion, `[in, out]` for variable subset, `|` for multiplex/switch\n",
    "\n",
    "* `sktime` provides easy-to-use extension templates for transformers, build your own, plug and play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Appendix - cheat sheets and extension guie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dunders glossary\n",
    "\n",
    "| Type | Dunder | Meaning | `sktime` class |\n",
    "| --- | --- | --- | --- |\n",
    "| compose | `*` | chaining/pipeline - also works with other estimator types | type dependent |\n",
    "| compose | `**` | chaining to secondary input of another estimator | type dependent |\n",
    "| compose | `+` | feature union | `FeatureUnion` |\n",
    "| interface | `~` | invert | `InvertTransform` |\n",
    "| structural | `Â¦` | multiplexing (\"switch\") | type dependent |\n",
    "| structural | `-` | optional passthrough (\"on/off\") | `OptionalPassthrough` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selected useful transformers, compositors, adapters\n",
    "\n",
    "* delay fitting to `transform` via `sktime.transformations.compose.FitInTransform`\n",
    "* any `pandas` method via `sktime.transformations.compose.adapt.PandasTransformAdaptor`\n",
    "* date/time features via `sktime.transformations.series.date.DateTimeFeatures`\n",
    "* lags via `transformations.series.lag.Lag`\n",
    "* differences, first and n-th, via `transformations.series.difference.Differencer`\n",
    "* scaled logit via `transformations.series.scaledlogit.ScaledLogitTransform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension guide - implementing your own transformer<a class=\"anchor\" id=\"chapter4\"></a>\n",
    "\n",
    "`sktime` is meant to be easily extensible, for direct contribution to `sktime` as well as for local/private extension with custom methods.\n",
    "\n",
    "To extend `sktime` with a new local or contributed transformer, a good workflow to follow is:\n",
    "\n",
    "1. read through the [transformer extension template](https://github.com/alan-turing-institute/sktime/blob/main/extension_templates/transformer.py) - this is a `python` file with `todo` blocks that mark the places in which changes need to be added.\n",
    "2. optionally, if you are planning any major surgeries to the interface: look at the [base class architecture](https://github.com/alan-turing-institute/sktime/blob/main/sktime/transformations/base.py) - note that \"ordinary\" extension (e.g., new algorithm) should be easily doable without this.\n",
    "3. copy the transformer extension template to a local folder in your own repository (local/private extension), or to a suitable location in your clone of the `sktime` or affiliated repository (if contributed extension), inside `sktime.transformations`; rename the file and update the file docstring appropriately.\n",
    "4. address the \"todo\" parts. Usually, this means: changing the name of the class, setting the tag values, specifying hyper-parameters, filling in `__init__`, `_fit`, `_transform`, and optional methods such as `_inverse_transform` or `_update` (for details see the extension template). You can add private methods as long as they do not override the default public interface. For more details, see the extension template.\n",
    "5. to test your estimator manually: import your estimator and run it in the worfklows in Section 2.2; then use it in the compositors in Section 2.3.\n",
    "6. to test your estimator automatically: call `sktime.tests.test_all_estimators.check_estimator` on your estimator. You can call this on a class or object instance. Ensure you have specified test parameters in the `get_test_params` method, according to the extension template.\n",
    "\n",
    "In case of direct contribution to `sktime` or one of its affiliated packages, additionally:\n",
    "* add yourself as an author to the code, and to the `CODEOWNERS` for the new estimator file(s).\n",
    "* create a pull request that contains only the new estimators (and their inheritance tree, if it's not just one class), as well as the automated tests as described above.\n",
    "* in the pull request, describe the estimator and optimally provide a publication or other technical reference for the strategy it implements.\n",
    "* before making the pull request, ensure that you have all necessary permissions to contribute the code to a permissive license (BSD-3) open source project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Credits: notebook 2 - transformers\n",
    "\n",
    "notebook creation: fkiraly\n",
    "\n",
    "transformer pipelines & compositors: fkiraly, mloning, miraep8\\\n",
    "forecaster pipelines: fkiraly, aiwalter\\\n",
    "classifier/regressor pipelines: fkiraly\\\n",
    "transformer base interface: mloning, fkiraly\\\n",
    "dunder interface: fkiraly, miraep8\n",
    "\n",
    "Based on design ideas: sklearn, magrittr, mlr, mlj"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pydata22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e61b44dca3bf47c8973c8cd627825697e2dad493e19dd6592afda0a0a3c312a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
