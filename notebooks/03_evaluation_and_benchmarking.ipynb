{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 Benchmarking with `sktime`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbd704",
   "metadata": {},
   "source": [
    "this notebook: setting up reproducible forecasting benchmarking experiments with `sktime`.\n",
    "\n",
    "A benchmarking experiment is specified by:\n",
    "\n",
    "* one or multiple models, possibly highly composite pipelines as before!\n",
    "* evaluation metrics, e.g., MAPE, CRPS\n",
    "* data sets, e.g., M5 collection\n",
    "* re-sampling setup, e.g., expanding window splitter with certain parameters\n",
    "* fit/update specification\n",
    "* possibly, post-hoc analyses on results of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f77c01",
   "metadata": {},
   "source": [
    "for *reproducible* benchmarking, need to pass on the above information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c818a8",
   "metadata": {},
   "source": [
    "`sktime` makes this easy via:\n",
    "\n",
    "* persisting blueprints of composites, metrics, re-sampling set-ups\n",
    "* persisting fitted estimators if required\n",
    "* standard data access interfaces for common benchmark data\n",
    "* few-line set-up of benchmarking experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f899fc8",
   "metadata": {},
   "source": [
    "explained below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31600872",
   "metadata": {},
   "source": [
    "option 1:\n",
    "\n",
    "* python environment versions\n",
    "* jupyter notebook with experiment\n",
    "* any code for custom estimator classes\n",
    "\n",
    "option 2:\n",
    "\n",
    "* python environment versions\n",
    "* list of persisted object blueprints - estimators, metrics, cv\n",
    "* benchmark experiment setup params\n",
    "* any code for custom estimator classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1750d80",
   "metadata": {},
   "source": [
    "## 3.1 Persisting models and objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ba3ae",
   "metadata": {},
   "source": [
    "for reproducibility, one may like to share:\n",
    "\n",
    "* model blueprint specs, e.g., equivalent of spec `Pipeline([(\"foo\", Foo()), (\"bar\", Bar(42))])`\n",
    "* fitted models, e.g., state of `my_pipe.fit(y)` after the `fit` - specific to data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d16ee8",
   "metadata": {},
   "source": [
    "### 3.1.1 Persisting model blueprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d733376",
   "metadata": {},
   "source": [
    "blueprint specs can be serialized using simple string print - this contains all information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40207180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define an example pipeline\n",
    "from sktime.forecasting.compose._pipeline import TransformedTargetForecaster\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "pipe = TransformedTargetForecaster(\n",
    "    steps=[\n",
    "        (\"imputer\", Imputer()),\n",
    "        (\"forecaster\", NaiveForecaster()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0991da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the pipeline to a string\n",
    "# this is useful for logging and sharing\n",
    "# pipe_str can be saved to a file, database, or shared over the internet\n",
    "pipe_str = str(pipe)\n",
    "pipe_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27604a73",
   "metadata": {},
   "source": [
    "for pseudo-random determinism, set any `random_state` parameters in the estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec360734",
   "metadata": {},
   "source": [
    "to deserialize, use `registry.craft` in the same python environment\n",
    "\n",
    "for python environment, e.g., use `pip freeze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import craft\n",
    "\n",
    "pipe_new = craft(pipe_str)\n",
    "pipe_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfffddf",
   "metadata": {},
   "source": [
    "this is the same estimator blueprint as `pipe`!\n",
    "\n",
    "To compare blueprint, simply use the `==` operator (this is a `scikit-base` feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_new == pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7ed2a",
   "metadata": {},
   "source": [
    "share complex pipelines like this with your researcher friends (or in the appendix of your publications)!\n",
    "\n",
    "I.e., process as follows:\n",
    "\n",
    "* publishing researcher shares `pipe_str = str(pipe)` or `str(my_estimator)` and `pip freeze > requirements.txt` output\n",
    "* reproducing researcher installs env from `requirements.txt` and runs `craft(pipe_str)` in that env\n",
    "\n",
    "For custom estimators, in addition, the custom module needs to be shared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0672d7",
   "metadata": {},
   "source": [
    "Highly complex estimators can consist of multiple definition blocks - this is also supported by `craft` as follows.\n",
    "\n",
    "Instead of a string conversion, we can also serialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b721f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_spec is a string representation of the pipeline\n",
    "# it can be stored in a file or a database like this\n",
    "# the \"return\" statement indicates which object we store\n",
    "# temporary variables like pipe, cv can be defined\n",
    "pipe_spec = \"\"\"\n",
    "pipe = TransformedTargetForecaster(steps=[\n",
    "    (\"imputer\", Imputer()),\n",
    "    (\"forecaster\", NaiveForecaster())])\n",
    "cv = ExpandingWindowSplitter(\n",
    "    initial_window=24,\n",
    "    step_length=12,\n",
    "    fh=[1, 2, 3])\n",
    "\n",
    "return ForecastingGridSearchCV(\n",
    "    forecaster=pipe,\n",
    "    param_grid=[{\n",
    "        \"forecaster\": [NaiveForecaster(sp=12)],\n",
    "        \"forecaster__strategy\": [\"drift\", \"last\", \"mean\"],\n",
    "    },\n",
    "    {\n",
    "        \"imputer__method\": [\"mean\", \"drift\"],\n",
    "        \"forecaster\": [ThetaForecaster(sp=12)],\n",
    "    },\n",
    "    {\n",
    "        \"imputer__method\": [\"mean\", \"median\"],\n",
    "        \"forecaster\": [ExponentialSmoothing(sp=12)],\n",
    "        \"forecaster__trend\": [\"add\", \"mul\"],\n",
    "    },\n",
    "    ],\n",
    "    cv=cv,\n",
    "    n_jobs=-1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d58a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "craft(pipe_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5019fa",
   "metadata": {},
   "source": [
    "sometimes, estimators require soft dependencies to be installed,\n",
    "\n",
    "and complain at construction (or `craft`)\n",
    "\n",
    "for this, required dependencies can be queried *before* construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import deps\n",
    "\n",
    "deps(pipe_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc98de",
   "metadata": {},
   "source": [
    "... although this should not be necessary if `pip freeze` output is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87929231",
   "metadata": {},
   "source": [
    "another useful convenience: `imports` can be used to print a full import block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864dd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import imports\n",
    "\n",
    "imports(pipe_spec)  # the result can be copied above the spec in to a jupyter cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39fe09",
   "metadata": {},
   "source": [
    "### 3.1.2 Persisting fitted models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2f784",
   "metadata": {},
   "source": [
    "persisting fitted models can be useful to share in a reproducibility setting,\n",
    "\n",
    "(note, data source plus blueprint with `random_state` may be easier to share)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab450c88",
   "metadata": {},
   "source": [
    "to persist a fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_airline\n",
    "\n",
    "y = load_airline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a159e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example pipeline\n",
    "from sktime.forecasting.compose._pipeline import TransformedTargetForecaster\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "pipe = TransformedTargetForecaster(\n",
    "    steps=[\n",
    "        (\"imputer\", Imputer()),\n",
    "        (\"forecaster\", NaiveForecaster()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.fit(y, fh=[1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a47e1",
   "metadata": {},
   "source": [
    "to serialize fitted objects, use `save` - default is `pkl`, but may differ for deep learning\n",
    "\n",
    "* no args produces in-memory object\n",
    "* `str` or `Path` arg will serialize to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bdb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_mem = pipe.save()\n",
    "# pipe_mem is a pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0889ecc",
   "metadata": {},
   "source": [
    "to deserialize use the `load` method on the memory object or a `str`, `Path`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b24ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.base import load\n",
    "\n",
    "pipe_new = load(pipe_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255919c",
   "metadata": {},
   "source": [
    "the loaded object can be used for prediction now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dd21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_new.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 Forecast evaluation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 Metrics for Point Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5a27d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 Metrics for Probabilistic Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103ac7a",
   "metadata": {},
   "source": [
    "## 3.3 Benchmarking - comparing estimator performance\n",
    "\n",
    "The `benchmarking` modules allows you to easily orchestrate benchmarking experiments in which you want to\n",
    "compare the performance of one or more algorithms over one or more datasets and benchmark configurations.\n",
    "\n",
    "Benchmarking as an endevour in general is very easy to get wrong, giving false conclusions about estimator\n",
    "performance - see this [2022 research from Princeton](https://reproducible.cs.princeton.edu/)\n",
    "for numerous examples of such mistakes in peer reviewed academic papers as evidence of this.\n",
    "\n",
    "`sktime`'s `benchmarking` module is designed to provide benchmarking functionality while enforcing best\n",
    "practices and structure to help users avoid making mistakes (such as data leakage, etc.) which invalidate\n",
    "their results. The `benchmarking` module is designed for easy usage in mind, as such it interfaces\n",
    "directly with `sktime` objects and classes. Previously developed estimator should be usable as they are without\n",
    "alterations.\n",
    "\n",
    "This notebook demonstrates usage of the `benchmarking` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.benchmarking.forecasting import ForecastingBenchmark\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.model_selection import ExpandingWindowSplitter\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.performance_metrics.forecasting import MeanSquaredPercentageError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b19c7",
   "metadata": {},
   "source": [
    "### Instantiate an instance of a benchmark class\n",
    "In this example we are comparing forecasting estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed25ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = ForecastingBenchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd5032",
   "metadata": {},
   "source": [
    "### Add competing estimators\n",
    "We add different competing estimators to the benchmark instance. All added estimators will \n",
    "be automatically ran through each added benchmark tasks, and their results compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9122963",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.add_estimator(\n",
    "    estimator=NaiveForecaster(strategy=\"mean\", sp=12),\n",
    "    estimator_id=\"NaiveForecaster-mean-v1\",\n",
    ")\n",
    "benchmark.add_estimator(\n",
    "    estimator=NaiveForecaster(strategy=\"last\", sp=12),\n",
    "    estimator_id=\"NaiveForecaster-last-v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30056814",
   "metadata": {},
   "source": [
    "### Add benchmarking tasks\n",
    "These are the prediction/validation tasks over which every estimator will be tested and their results compiled.\n",
    "\n",
    "The exact arguments for a benchmarking task depend on the whether the objective is forecasting, classification, etc.,\n",
    "but generally they are similar. The following are the required arguments for defining a forecasting benchmark task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368d276",
   "metadata": {},
   "source": [
    "#### Specify cross-validation split regime(s)\n",
    "Define cross-validation split regimes, using standard `sktime` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812bd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_splitter = ExpandingWindowSplitter(\n",
    "    initial_window=24,\n",
    "    step_length=12,\n",
    "    fh=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e2a07",
   "metadata": {},
   "source": [
    "#### Specify performance metric(s)\n",
    "Define performance metrics on which to compare estimators, using standard `sktime` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = [MeanSquaredPercentageError()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b66d3",
   "metadata": {},
   "source": [
    "#### Specify dataset loaders\n",
    "Define dataset loaders, which are callables (functions) which should return a dataset. Generally\n",
    "this is a callable which returns a dataframe containing the entire dataset. One can use\n",
    "the `sktime` defined datasets, or define their own. Something as simple as the following\n",
    "example will suffice: \n",
    "```python\n",
    "def my_dataset_loader():\n",
    "    return pd.read_csv(\"path/to/data.csv\")\n",
    "```\n",
    "The datasets will be loaded when running the benchmarking tasks, ran through the cross-validation\n",
    "regime(s) and subsequently the estimators will be tested over the dataset splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loaders = [load_airline]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499be64",
   "metadata": {},
   "source": [
    "#### Add tasks to the benchmark instance\n",
    "Use the previously defined objects to add tasks to the benchmark instance.\n",
    "Optionally use loops etc. to easily setup multiple benchmark tasks reusing arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_loader in dataset_loaders:\n",
    "    benchmark.add_task(\n",
    "        dataset_loader,\n",
    "        cv_splitter,\n",
    "        scorers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401deb16",
   "metadata": {},
   "source": [
    "### Run all task-estimator combinations and store results\n",
    "\n",
    "Note that `run` won't rerun tasks it already has results for, so adding a new\n",
    "estimator and running `run` again will only run tasks for that new estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = benchmark.run(\"./forecasting_results.csv\")\n",
    "results_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Credits: notebook 3 - Metrics and Evaluation\n",
    "\n",
    "notebook creation:\n",
    "\n",
    "forecaster pipelines:\n",
    "transformer pipelines & compositors:\n",
    "dunder interface:\n",
    "\n",
    "tuning, autoML:\n",
    "CV and splitters:\n",
    "forecasting metrics:\n",
    "backtesting, evaluation:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
